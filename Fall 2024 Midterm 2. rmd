---
title: "Midterm Exam 2 - Open Book Section (R) - Part 2"
output:
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Instructions

1.  Save the .Rmd/.ipnyb file in your working directory - the same
    directory where you will download the data files into.

2.  Read the question and create the code necessary within the code
    chunk section immediately below each question.

3.  Type your answer to the questions in the text block provided
    immediately after the response prompt.

4.  Once you've finished answering all questions, knit this file and
    submit the knitted file *as HTML* on Canvas.

    ```         
    * Make sure to start submission of the exam at least 10 minutes before the end of the exam time. It is your responsibility to keep track of your time and submit before the time limit. 

    * If you are unable to knit your file as HTML for whatever reason, you may upload your Rmd/ipynb/PDF/Word file instead. However, you will be penalized 15%.

    * If you are unable to upload your exam file for whatever reason, you may IMMEDIATELY attach the file to the exam page as a comment via Grades-> Midterm Exam 2 - Open Book Section - Part 2 -> Comment box. 

    * Note that you will be penalized 15% (or more) if the submission is made within 5 minutes after the exam time has expired and a higher penalty if more than 5 minutes. Furthermore, you will receive zero points if the submission is made after 15 minutes of the exam time expiring. We will not allow later submissions or re-taking of the exam.

    * If you upload your file after the exam closes, let the instructors know via a private Piazza post. Please DON'T attach the exam file via a private Piazza post to the instructors since you could compromise the exam process. Any submission received via Piazza will not be considered.

    *#Commented out code will be graded for partial credit and the submitted file must be HTML
    ```

**Ready? Let's begin.**

### **For questions 1-4: Use the dataset "employee_turnover_prediction".**

### **For question 5: Use the dataset "quine_dataset". This dataset was taken from the MASS library.**

## Background

In this exam, you will be considering various attributes to predict
**employee turnover** (whether an employee will leave or stay) based on
various factors:

1.  **Commute Mode:** The mode of transportation the employee uses to
    commute to work.

    ("Public Transport," "Car," "Bike," "Walk") **(Qualitative
    variable)**

2.  **Workplace Flexibility:** The level of flexibility the employee has
    in their work location. ("Remote," "Hybrid," "On-Site")
    **(Qualitative variable)**

3.  **Team Dynamics:** The level of collaboration within the employee's
    team. ("High Collaboration," "Low Collaboration") **(Qualitative
    variable)**

4.  **Office Location:** The geographical location of the office where
    the employee works. ("City Center," "Suburb," "Rural Area")
    **(Qualitative variable)**

5.  **Health Benefits:** The type of health benefits provided to the
    employee by the company. ("Full Coverage," "Partial Coverage,"
    "None") **(Qualitative variable)**

6.  **Satisfaction Score:** A rating of the employee’s overall
    satisfaction with their job.(1-10) **(Quantitative variable)**

7.  **Monthly Working Hours:** The total number of hours worked by the
    employee in a typical month. (120-250) **(Quantitative variable)**

8.  **Years With Company:** The number of years the employee has worked
    at the company. (0.5-35 years)**(Quantitative variable)**

9.  **Number of Trainings Attended:** The number of professional
    development or training sessions the employee has participated in.
    (0-10) **(Quantitative variable)**

10. **Salary Increase Percentage:** The percentage increase in the
    employee's salary over the past year.(0%-20%) **(Quantitative
    variable)**

11. **Turnover:** Indicates whether the employee left the company. "1"
    for Turnover (employee left), "0" for No Turnover (employee stayed)
    **(Response variable)**.

```{r}
#This seed has been set to 100 to ensure results are reproducible. DO NOT CHANGE THIS SEED
set.seed(100)

#Read the csv file
employee_turnover = read.csv("employee_turnover_prediction.csv", header=TRUE, na.strings = "")

#Remove any potential trailing white space from column names
names(employee_turnover) <- trimws(names(employee_turnover), which = "right")

employee_turnover$Commute_Mode=as.factor(employee_turnover$Commute_Mode)
employee_turnover$Workplace_Flexibility=as.factor(employee_turnover$Workplace_Flexibility)
employee_turnover$Team_Dynamics=as.factor(employee_turnover$Team_Dynamics)
employee_turnover$Office_Location=as.factor(employee_turnover$Office_Location)
employee_turnover$Health_Benefits=as.factor(employee_turnover$Health_Benefits)
employee_turnover$Turnover=factor(employee_turnover$Turnover, levels = c(0, 1))

#Dividing the dataset into training and testing datasets
testRows = sample(nrow(employee_turnover),0.2*nrow(employee_turnover))
testData = employee_turnover[testRows, ]
trainData = employee_turnover[-testRows, ]
row.names(trainData) <- NULL
head(trainData) #display train data
```

## Question 1: Data Exploration (11 points)

**Use the FULL dataset "employee_turnover" for Question 1**

**1a) (2 points) What is the median "Monthly_Working_Hours" for
employees across different workplaces?**

***Note*****: Answer must be grouped by "Workplace_Flexibility".**

```{r}
#Code
library(dplyr)
median_hours = aggregate(Monthly_Working_Hours ~ Workplace_Flexibility, data = employee_turnover, FUN = median)
print(median_hours)
```

**1b) (2 points) What is the proportion of employees who stayed with the
company (i.e., did not leave) for each type of "Health_Benefits"?**

***Note*****: As an example, the proportion of employees who stayed with
the company for Full Coverage equals the number of employees with full
coverage who stayed divided by the number of employees with full
coverage.**

```{r}
#Code
proportion_stayed = employee_turnover %>% 
  group_by(Health_Benefits) %>%
  summarise(stayed_proportion = sum(Turnover==0)/n())
print(proportion_stayed)
```

**1c) (2 points) Print the rows with the highest
"Salary_Increase_Percentage". Identify the qualitative variable
responses that are the same between the rows with the highest
"Salary_Increase_Percentage"?**

```{r}
#Code
highest_salary_increase = employee_turnover %>%
  filter(Salary_Increase_Percentage == max(Salary_Increase_Percentage))

print(highest_salary_increase)

```

**Answer to question 1c**: Public Transport and Car are the rows with the highest "Salary_Increase_Percentage".

**1d) (5 points) Create boxplots and interpret each plot for the
following predictors against the response variable (Turnover).**

**i) Monthly_Working_Hours**

**ii) Years_With_Company**

**In general, using boxplots, can we make statements about statistical
significance of the differences between the group means? How can we
infer if the group means are statistically significantly different from
each other?**

```{r}
#Code
boxplot(employee_turnover$Monthly_Working_Hours ~ employee_turnover$Turnover,
        main = "Monthly Working Hours by Turnover", 
        xlab = "Turnover", ylab = "Monthly Working Hours")

boxplot(employee_turnover$Years_With_Company ~ employee_turnover$Turnover,
        main = "years with company by Turnover", 
        xlab = "Turnover", ylab = "Years with Company")
```

**Answer to question 1d**:
No, Box plots alone do not allow us to make definitive statements about statistical significance. we infer if the group means are statistically significantly different from each other by using statistal test such as ANOVA, t-test, etc.
  
## Question 2: Logistic Regression Model (17 points)

**2a) (6 points) In this question, you will fit a reduced model:**

**i) Using the dataset “trainData”, create a logistic regression model
(call it "*model1*") using "Turnover" as the response variable, and
Office_Location, Health_Benefits, Monthly_Working_Hours as the predictor
variables. (2 points)**

```{r}

model1 = glm(Turnover ~ Office_Location + Health_Benefits + Monthly_Working_Hours, data = trainData,family = binomial)
summary(model1)
```

**ii) (2 points) Using "model1",  interpret the coefficients of the
following predictors below with respect to BOTH the log-odds of turnover
and the odds of turnover.**

**1) Monthly working hours 2) Health_BenefitsPartial Coverage.**

**Answer to Question 2a(ii):**

Monthly working hours coefficient is -0.002089

For each additional hour worked per month the log odds of turnover decrease by 0.002089 holding all other variables constant, this is the log odds interpretation. The odds interpretation would be e ^ -0.002089 which equals approximately 0.9979 which means that for each additional hour worked per month the odds of turnover are multiplied by approximately 0.9979

Health_BenefitsPartial coefficient is -0.177077. 

Free traditional hour worked per month. The log audits went over decreased by 0.177077 holding all variables constant. This is the odds interpretation. The odds interpretation would be e^ 0.177077 which is approximately 0.8375 which means that for each additional hour worked per month the odd turnover are multiplied by approximately is 0.8375


**iii) Is the model with all the predictors better at predicting the
response than a model with just an intercept term? Explain your
reasoning. (2 points)**

***Note*****: You can use only the summary output for model1 to answer
this question.**

**Answer to Question 2a(iii):**
If the predictors perfectly predict the response, the model could potentially rely only on an intercept term, but this can lead to overfitting or perfect grouping issues. Regularization or reducing predictors is sometimes necessary to reduce over fitting and improve model stability.

**2b) (4 points) In this question, you will fit the full model:**

**i) (2 points) Using the "trainData" dataset, create a logistic
regression model using Turnover as response variable and all variables
in "trainData" as predictors (call it model2) and display the summary of
model2.** 

```{r}
model2 = glm(Turnover ~ ., data = trainData, family = binomial)
summary(model2)
```

**ii)(2 points) Compare the full logistic regression model (model2) from
Question (2bi) against the reduced model (model1) from Question (2ai).
What can you conclude from the results of this comparison using a
significance level of alpha=0.01?**

```{r}
anova(model1, model2, test = "Chisq")

```

**Response to question (2b(ii))**: We can see that that the degrees of freedom in model 1 is 1287 while the degrees of freedom in model 2 is 1277. There is a 10 df  difference both models resulting in different deviations. We can conclude that we get more information for model two considering it has more predictors contributing to the model. 

**2c) (2 points) Perform a test for overall regression of the logistic
regression "model2", using a significance level of alpha=0.05. Does the
overall regression have explanatory power? Provide interpretation of the
test.**

```{r}

anova(model2, test = "Chisq")
```

**Response to question (2c)**: I believe model 2 does not have significant explanatory power because only two of the coefficient P values are lower than the alpha of 0.05 and those two are workplace flexibility, and Office location.

**2d)(5 points) Using "model2", apply hypothesis testing for a
regression coefficient at the 0.01 significance level.**

**i) (1 point) Is the coefficient of “Number_of_Trainings_Attended”
statistically significant?**

#**ii) (1 point) State the Null and alternative hypotheses of the test.**

**iii) (1.5 points) Describe the approach we would use to determine the
statistical significance of the regression coefficient.**

**iv) (1.5 points) What is the sampling distribution that the test
statistic follows?** 

**Response to question 2d(i):**
Yes, the coefficient of numbers of training attendant is the statistically significant at -0.0313203 which is less than the alpha of 0.01.

**Response to question 2d(ii):**
NULL hypothesis for the coefficient for numbers of trainings attended is equal to zero and this would imply that this has no effect on the likelihood of tone over while the alternative hypothesis would be that coefficient for numbers of training attendant is not equal to zero, which means that it has a statistically significant impact on turnover.

**Response to question 2d(iii):**
The statistical significance is determined by its P value if the P value is below the chosen significance level (a.k.a. alpha, in this case is 0.01) we would reject the hypothesis hypothesis, concluding that the predictor is statistically significant if it is above, then it is insignificant.
**Response to question 2d(iv):**
The test Stasik for the coefficient and logistic regression follows a standard normal distribution Z distribution under the no hypothesis therefore, we use the normal Z distribution to calculate the P values. 

## Question 3: Decision Tree and Random Forest Models (6 points)

**3a) (4 points) Using the dataset "trainData", fit the following
classification models below using all the predictors in "trainData" and
"Turnover" as the response variable.**

**i) Decision Tree Model (call it *model3*).**

**ii) Random Forest Model (call it *model4*).**

**Use metric = “Accuracy”, trControl = trainControl(method=“cv”,
number=3) for both models. Display the summary of both models and state
the average accuracy for both resampled models.**

```{r}
library(caret)
```

```{r}
# Decision Tree

model3 = train(Turnover ~ ., data = trainData, method = "rpart",
               trControl = trainControl(method="cv", number =3 ))
model3


```

```{r}
# Random Forest

model4 = train(Turnover ~ ., data = trainData, method = "rf",
               trControl = trainControl(method="cv", number =3 ))
model4

```

**3b) (2 points) Which model performed better when comparing the average
accuracy of the resampled decision tree and random forest models?
Explain the difference between the decision tree model and the random
forest model.**

**Answer to Question 3b**
random forest, which is model 4 performs better because it improves the ability and accuracy the decision tree model, which is model 3 is simpler and faster to interpret, but is often less accurate because it cannot handle such complex data. The random forest reduces overfitting and variance and combines multiple decision trees leasding to better results.

## Question 4: Prediction (14 points)

**Use the "testData" for all questions in this question.**

**4a)(4 points) Using testData, predict the probability of an employee
leaving, i.e. being a turnover, and output the AVERAGE of these
probabilities for each of the models below:**

**i) model1 (question 2a) ii) model2 (question 2b) iii) model3 (question
3a) and iv) model4 (question 3a)**

```{r}
probmodel1 = predict(model1, testData, type = "response")
probmodel2 = predict(model2, testData, type = "response")
probmodel3 = predict(model3, testData, type = "prob")[,2]
probmodel4 = predict(model4, testData, type = "prob")[,2]

#probmodel1
#probmodel2
#probmodel3
#probmodel4

mean_probabilities = data.frame(
  Model1AVGP = mean(probmodel1),
  Model2AVGP = mean(probmodel2),
  Model3AVGP = mean(probmodel3),
  Model4AVGP = mean(probmodel4)
  
)

print(mean_probabilities)

```

**4b) (4 points) Using the probabilities from Q4a and a threshold of 0.5
(inclusive of 0.5), obtain the classifications of an employee being a
turnover for all four models.Note: every row in the testData prediction
must be classified.Print the last ten classification rows for all the
model classifications as well as the actual response for Turnover of
those rows.**

```{r}

CLASSMODEL1 = ifelse(probmodel1 >= 0.5,1,0)
CLASSMODEL2 = ifelse(probmodel2 >= 0.5,1,0)
CLASSMODEL3 = ifelse(probmodel3 >= 0.5,1,0)
CLASSMODEL4 = ifelse(probmodel4 >= 0.5,1,0)

classificationresults = data.frame(
  Actualturnover =testData$Turnover,
  Model1classification = CLASSMODEL1,
  Model2classification = CLASSMODEL2,
  Model3classification = CLASSMODEL3,
  Model4classification = CLASSMODEL4

)

tail(classificationresults,10)


```

**4c) (6 points) In this question, you will compare the prediction
accuracy of the four models.\
i) (4 points) Using the classifications from Q4b, create a confusion
matrix and output the classification evaluation metrics (i.e. Accuracy,
Sensitivity, and Specificity) for all four models. Note: every row in
the testData classification must be used (do not use only the last ten
classification rows).**"

**ii) (2 points) Which metric measures the rate of true negatives? Which
model shows the highest value for this metric?** 

```{r}
actual = testData$Turnover

confusionmodel1 = confusionMatrix(factor(CLASSMODEL1),factor(actual))
confusionmodel2 = confusionMatrix(factor(CLASSMODEL2),factor(actual))
confusionmodel3 = confusionMatrix(factor(CLASSMODEL3),factor(actual))
confusionmodel4 = confusionMatrix(factor(CLASSMODEL4),factor(actual))

cat("Model 1 metrics: \n")
print(confusionmodel1)
cat("Model 2 metrics: \n")
print(confusionmodel2)
cat("Model 3 metrics: \n")
print(confusionmodel3)
cat("Model 4 metrics: \n")
print(confusionmodel4)

```

**Response to question 4c(ii) :**
Specificity measures the rate of true negatives and model 4 shows the highest value for this metric at 0.9394.

## Question 5: Poisson Regression Model (12 points)

**Use the "quine_dataset" for Question 5.**

### Background

This data frame contains the following columns:

`Eth`

:   ethnic background: Aboriginal or Not, (`"A"` or `"N"`).

`Sex`

:   sex: factor with levels (`"F"` or `"M"`).

`Age`

:   age group: Primary (`"F0"`), or forms `"F1,"` `"F2"` or `"F3"`.

    -   **F0**: Primary school age (youngest children)

    -   **F1**: First-year high school (Form 1)

    -   **F2**: Second-year high school (Form 2)

    -   **F3**: Third-year high school (Form 3)

`Lrn`

:   learner status: factor with levels Average ("AL") or Slow learner
    ("SL")

`Days`

:   days absent from school in the year.

```{r}
# Read data
quine_dataset = read.csv("quine_dataset.csv", header=TRUE, na.strings = "")

# Remove any potential trailing white space from column names
names(quine_dataset) <- trimws(names(quine_dataset), which = "right")

# Show first few rows
head(quine_dataset)
```

**5a) (4 points)**

**i)(2 points) Plot a histogram of the count of "Days" from the
"quine_dataset"**

```{r}
# Check the distribution of the response, Days
 hist(quine_dataset$Days, main = "Histogram of Days absent", xlab= "days absent")

```

**ii)(2 points) Create boxplots of the response variable “Days” against
the predicting variable “Sex”. Explain the relationship between the
response variable and predicting variable based on the boxplot. Using
the boxplot only, do you observe any overlap or potential outliers?**

```{r}
#Code

boxplot(Days ~ Sex, data = quine_dataset, Main = "Days absent by sex", col=c("pink", "lightblue"))
```

**Response to Q5a(ii)**
Based on the box pot males have a higher average amount of absence days compared to females while females have a higher amount of outliers compared to males.

**5b) (4 points)\
i) Fit a poisson regression model using all the predictors from the
“quine_dataset” and “Days” as the response variable. Call
it pois_model1 and display the model summary**

**ii) Interpret the coefficient of “AgeF2” in pois_model1 with respect
to the log expected "Days".**

**iii) Interpret the coefficient of “EthN” in pois_model1 with respect
to the rate ratio of "Days".**

**iv) Why can't we use a standard regression model with the log
transformation of the response variable instead of creating a Poisson
regression model?**

```{r}
pois_model1 = glm(Days ~ ., family = poisson, data = quine_dataset)
summary(pois_model1)
```

**Response to question 5b(ii):**
The coefficient for AgeF2 in the poisson regression model1 is 0.25783. This coefficient explains the expected difference in absence days for students in this age group compared to the baseline. Since the coefficient is positive, this means that being in the AgeF2 group is associated with an increase in the log of expected numbers of days absent.

**Response to question 5b(iii):**
The coefficient of ethnicity in model1 is -0.53360 which means that being in this group is associated with the decrease in the expected numbers of day being absent.

**Response to question 5b(iv):**
Poisson regression is ideal compared to standard regression Models with the log transformation because it is ideal for count data as it models non-negative discrete outcomes and it accounts for mean and variance relationships within the data. 

**5c)(4 points)**

**i) Calculate the estimated dispersion parameter for "pois_model1"
using both the deviance and Pearson residuals. Is this an overdispersed
model using a threshold of 2.0? Justify your answer.**

**ii) Create a proposed model (call it "pois_model2") that handles
overdispersion using the quine_dataset.**

**iii) Explain the concept of overdispersion in Poisson regression and
discuss the potential causes of overdispersion.**

**iv) Describe how overdispersion can affect the reliability of
statistical inference in Poisson regression models.**

```{r}
deviancedispersion = deviance(pois_model1)/ df.residual(pois_model1)
pearsondispersion = sum(residuals(pois_model1, type = "pearson")^2) / df.residual(pois_model1)

cat("Dispersion(Deviance):",deviancedispersion, "\n")
cat("Dispersion(Pearson):",pearsondispersion, "\n")

```

**Response to Q5c(i)**
Since both dispersion parameters are greater than 2, the model is over disperse, which means that the poisson model may not be a good fit

Dispersion(Deviance): 12.20652 
Dispersion(Pearson): 13.16684

**Response to Q5c(ii)**

```{r}

pois_model2 = glm(Days ~ ., family = quasipoisson, data = quine_dataset)
summary(pois_model2)
```

**Response to Q5c(iii)**
Over dispersion happens when the variance of the response variable is bigger than the mean, which does not confine to the Poisson assumption that mean = variance

**Response to Q5c(iv)**
Overdispersion can affect the statistical inference in poisson regression models because it can lead to test statistics being too large and P values being too small. This can make us assume that certain predictors are significant when they truly are not :)
**End of exam**
